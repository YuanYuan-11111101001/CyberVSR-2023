{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fuVDIsYdOAc8",
        "SdzbVlDTUJK6",
        "p0ZGfiev9FCD",
        "6o-v-Dxk9Jr7",
        "Okp79-5A9OAz",
        "x-b1QH783U8V",
        "JnsWiuA2oYiv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuanYuan-11111101001/CyberVSR-2023/blob/main/Feature_Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eXrnaMghz-w",
        "outputId": "20f614fc-8a6c-4ef1-b8bb-dcf7f278d70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "import seaborn as sns\n",
        "import scipy.stats as ss\n",
        "# from scipy.stats import entropy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "5vdbFyFch4jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting timestamps to week number of the year. Remember to load dataset first\n",
        "date = pd.to_datetime(august['timestamp'], errors='coerce')\n",
        "\n",
        "date.astype('int64').dtypes\n",
        "\n",
        "weeknumbers = date.dt.week\n",
        "\n",
        "august['week'] = weeknumbers\n",
        "# week number 5-9 for February\n",
        "# 33 - 35 for August"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWtKy_lyqWrH",
        "outputId": "13cd33c0-0067-481c-957c-ed97e3729d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-9331564bd20e>:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
            "  weeknumbers = date.dt.week\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: separating dataset by week number\n",
        "week_33 = august[august['week'] == 33]\n",
        "week_34 = august[august['week'] == 34]\n",
        "week_35 = august[august['week'] == 35]"
      ],
      "metadata": {
        "id": "jOhVUnlErDha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = week_35"
      ],
      "metadata": {
        "id": "ZpQCqabiyNSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "bwcZLaW4N-UQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For username, password, and command:"
      ],
      "metadata": {
        "id": "fuVDIsYdOAc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_valid(df, columns):\n",
        "  '''\n",
        "  Returns the number of valid records in a dataset where a value is not considered\n",
        "  valid iff it is empty/NaN CHANGE to return serieSSS\n",
        "  '''\n",
        "  valid_numbers = {}\n",
        "  for column in columns:\n",
        "    x = df.dropna(subset = [column])\n",
        "    valid_numbers[column] = len(x)\n",
        "\n",
        "  return pd.Series(valid_numbers)"
      ],
      "metadata": {
        "id": "qEccJLi4N_sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def percent_valid(df, columns):\n",
        "  '''\n",
        "  returns percent of non-empty records in each column in columns\n",
        "  '''\n",
        "  percent_valid = {}\n",
        "  for column in columns:\n",
        "    percent_valid[column] = len(df.dropna(subset = [column]))/len(df)\n",
        "\n",
        "  return pd.Series(percent_valid)"
      ],
      "metadata": {
        "id": "slWO6STIRZyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_valid(df, columns):\n",
        "  '''\n",
        "  returns number of unique non-empty records in each column in columns\n",
        "  '''\n",
        "  valid_unique = {}\n",
        "  for column in columns:\n",
        "    valid_unique[column] = df[column].nunique(dropna=True)\n",
        "  return pd.Series(valid_unique)"
      ],
      "metadata": {
        "id": "oaDd3A4QR6XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Src_port, Dest_port, and sensor"
      ],
      "metadata": {
        "id": "SdzbVlDTUJK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_outliers(data, column, include_NaN=False):\n",
        "  '''\n",
        "  Identify outliers within a categorical field of a dataframe. Returns dataframe with outlier values.\n",
        "  Returns an empty dataframe if there are none.\n",
        "\n",
        "  An outlier is defined as a a value that falls outside three standard deviations of the mean.\n",
        "  We convert the categorical variables into the frequencies in which each unique value occurs\n",
        "  within that field. We define the mean as the mean frequency of occurrence and the standard\n",
        "  deviation as the standard deviation of frequencies. Values with frequencies that are not\n",
        "  within three standard deviations of the mean frequency will be returned in the dataframe\n",
        "  '''\n",
        "  frequencies = data[column].value_counts(normalize=True,dropna=include_NaN)\n",
        "\n",
        "  data['freq'] = frequencies\n",
        "\n",
        "  mean = frequencies.mean()\n",
        "  std = frequencies.std()\n",
        "\n",
        "  return data[(data['freq'] <= mean-3*std) | (data['freq'] >= mean+3*std)]"
      ],
      "metadata": {
        "id": "uAPkG9KsUIms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_outlier(data, column, outliers):\n",
        "\n",
        "  counter = 0\n",
        "  total = 0\n",
        "  for value in data[column]:\n",
        "    total += 1\n",
        "    if value in outliers:\n",
        "      counter += 1\n",
        "  # print(len(df))\n",
        "  # len(df) not working????\n",
        "  return counter/total"
      ],
      "metadata": {
        "id": "tRG-g4cMdrIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common(data, column):\n",
        "  '''\n",
        "  Return most common unique value in column\n",
        "  '''\n",
        "  # for column in columns:\n",
        "  #   most = data[column].value_counts(dropna=True).nlargest(3)\n",
        "  #   most['column_name'] = column\n",
        "  top_three = data[column].value_counts(dropna=True)[:3].index.tolist()\n",
        "\n",
        "\n",
        "  return top_three"
      ],
      "metadata": {
        "id": "iSZAmFTVgeQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_common(data, column, common):\n",
        "  counter = 0\n",
        "  for value in data[column]:\n",
        "    if value in common:\n",
        "      counter += 1\n",
        "\n",
        "  return counter\n"
      ],
      "metadata": {
        "id": "yyV7f8kNgk3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(df, columns, same_base=True):\n",
        "  '''\n",
        "  return entropy values of each column\n",
        "  '''\n",
        "  entropies = {}\n",
        "  for col in columns:\n",
        "      freq = df[col].value_counts(normalize=True,dropna=True)\n",
        "      if same_base:\n",
        "          # ent = -freq * np.log2(freq)\n",
        "          ent = freq * np.log2(freq)\n",
        "      else:\n",
        "          base =  df[col].nunique(dropna=False)\n",
        "          # ent = -freq * np.log2(freq)/np.log2(base)\n",
        "          ent = freq * np.log2(freq)/np.log2(base)\n",
        "      ent = -1 * ent.sum()\n",
        "      entropies[col] = ent\n",
        "  return pd.Series(entropies)"
      ],
      "metadata": {
        "id": "0CYS4xwGPgDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating cross entropies. Note that there are several versions of cross entropy to accomodate different RAM limits\n",
        "# To accomodate for google colab free version limited RAM, used get_p, qstep_1, and ce_steps\n",
        "def cross_entropy(df1, df2, columns):\n",
        "  '''\n",
        "  Where df1 is the true and df2 is pred.\n",
        "\n",
        "  Don't include NaN for fields\n",
        "  '''\n",
        "\n",
        "  freq_true = df1[columns].value_counts(normalize=True,dropna=False)\n",
        "  ent = freq_true * np.log2(df2)\n",
        "  ent = -1 * ent.sum()\n",
        "  return ent\n",
        "\n",
        "def cross_entropy_m(df1, df2, columns):\n",
        "  '''\n",
        "  Where df1 is the true and df2 is pred.\n",
        "\n",
        "  Don't include NaN for fields\n",
        "  '''\n",
        "  df2 = df1.copy()\n",
        "  df2 = df2[df2['src_ip'] != df1['src_ip']]\n",
        "  df2 = df2[columns].value_counts(normalize=True, dropna=False)\n",
        "\n",
        "  freq_true = df1[columns].value_counts(normalize=True,dropna=False)\n",
        "  ent = freq_true * np.log2(df2)\n",
        "\n",
        "  ent = -1 * ent.sum()\n",
        "  return ent\n",
        "\n",
        "def cross_entropy_combined(df1, df2):\n",
        "  df = df2.copy()\n",
        "  ent = df1*np.log2(df2)\n",
        "  ent = -1 * ent.sum()\n",
        "  df['ce'] = ent\n",
        "  return ent\n",
        "\n",
        "def get_p(df, column):\n",
        "  # Returns p distribution for calculating cross entropy\n",
        "  distribution = df[column].value_counts(normalize=True,dropna=True)\n",
        "  if distribution.empty:\n",
        "    return pd.Series({np.nan: 0})\n",
        "  else:\n",
        "    return distribution\n",
        "\n",
        "def get_q(df, dff, column):\n",
        "  q = dff[~dff['src_ip'].isin(df['src_ip'])]\n",
        "  return q[column].value_counts(normalize=True,dropna=True).tolist()\n",
        "\n",
        "def get_q1(df, dff, column):\n",
        "  return dff[~dff['src_ip'].isin(df['src_ip'])]\n",
        "\n",
        "def get_q2(df, column):\n",
        "  return df[column].value_counts(normalize=True,dropna=True)\n",
        "\n",
        "def get_qstep1(df, df2, column):\n",
        "  '''\n",
        "  Returns normalized (divide by total size) df2 minus the value counts of df[column].\n",
        "  where df is the dataframe and df2 is a value_counts list of distribution q.\n",
        "  '''\n",
        "  df_ret = df2.copy()\n",
        "  values = df[column].value_counts(dropna = True)\n",
        "  for value in values.keys():\n",
        "    df_ret.loc[value] = df2.loc[value] -values.loc[value]\n",
        "  return df2/(df2.sum())\n",
        "\n",
        "def ce_steps(df1, df2, name_of_ce):\n",
        "  '''\n",
        "  returns dataframe of each source ip and it's cross entropy. where df1 is p\n",
        "  and df2 is q.\n",
        "  '''\n",
        "  c = []\n",
        "  ind = []\n",
        "  for i in df2.index:\n",
        "    ent = df1.loc[i]*np.log2(df2.loc[i].dropna())\n",
        "    ent = -1*ent.sum()\n",
        "\n",
        "    ind.append(i)\n",
        "    c.append(ent)\n",
        "\n",
        "  data = {'src_ip':ind, 'ce ' + name_of_ce: c}\n",
        "\n",
        "  ces = pd.DataFrame(data)\n",
        "  ces = ces.set_index('src_ip')\n",
        "\n",
        "  return ces\n",
        "\n",
        "def cross_entropy_base(df1, df2, columns):\n",
        "  freq_true = df1[columns].value_counts(normalize=True,dropna=False)\n",
        "\n",
        "  # ent = -freq_true * np.log2(df2)\n",
        "  base =  len(df2)\n",
        "  ent = -freq_true * np.log2(df2)/np.log2(base)\n",
        "\n",
        "  ent = ent.sum()\n",
        "  return ent"
      ],
      "metadata": {
        "id": "8xI2XPIWzybT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Transformation:"
      ],
      "metadata": {
        "id": "p0ZGfiev9FCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_src = df.groupby('src_ip').size()\n",
        "attack_freq = pd.DataFrame(unique_src)\n",
        "attack_freq.columns=['attack_frequency']\n",
        "attack_freq"
      ],
      "metadata": {
        "id": "lj_Sg8e_n0tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate entropies"
      ],
      "metadata": {
        "id": "6o-v-Dxk9Jr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entropy_columns = ['src_port', 'dest_port', 'sensor', 'username', 'ssh_username', 'command']"
      ],
      "metadata": {
        "id": "e5I3fYbETy0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entropy_dataset = df.groupby('src_ip')[entropy_columns].apply(entropy, columns=entropy_columns)\n",
        "new_col_names = ['entropy '+ i for i in entropy_dataset.columns]\n",
        "entropy_dataset.columns = new_col_names\n",
        "# Print the results\n",
        "entropy_dataset"
      ],
      "metadata": {
        "id": "pkqVYFKaPz1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group One: username, ssh_username, command"
      ],
      "metadata": {
        "id": "Okp79-5A9OAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_one = ['username', 'ssh_username', 'command']"
      ],
      "metadata": {
        "id": "9aNw2R7r5E3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_valid_df = df.groupby(['src_ip'])[group_one].apply(num_valid, columns = group_one)\n",
        "\n",
        "new_col_names_nv = ['num_valid '+ i for i in number_valid_df.columns]\n",
        "number_valid_df.columns = new_col_names_nv"
      ],
      "metadata": {
        "id": "5k6002DU4FiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percent_valid_df = df.groupby(['src_ip'])[group_one].apply(percent_valid, columns = group_one)\n",
        "\n",
        "new_col_names_pv = ['percent_valid '+ i for i in percent_valid_df.columns]\n",
        "percent_valid_df.columns = new_col_names_pv"
      ],
      "metadata": {
        "id": "oVgrAtlVBAW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_valid_df = df.groupby(['src_ip'])[group_one].apply(unique_valid, columns = group_one)\n",
        "\n",
        "new_col_names_uv = ['unique_valid '+ i for i in unique_valid_df.columns]\n",
        "unique_valid_df.columns = new_col_names_uv"
      ],
      "metadata": {
        "id": "VkHtlGQfl8BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Two: src_port, dest_port, sensor"
      ],
      "metadata": {
        "id": "x-b1QH783U8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_two = ['src_port', 'dest_port', 'sensor']"
      ],
      "metadata": {
        "id": "sZ8Qa59T3ag2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find Outlier Lists\n",
        "src_outliers = find_outliers(df, 'src_port', True)\n",
        "dest_outliers = find_outliers(df, 'dest_port', True)\n",
        "sensor_outliers = find_outliers(df, 'sensor', True)\n",
        "\n",
        "src_outliers = src_outliers[['src_port']]\n",
        "dest_outliers = dest_outliers[['dest_port']]\n",
        "sensor_outliers = sensor_outliers[['sensor']]"
      ],
      "metadata": {
        "id": "QAZtHZXh3gdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a00b889-9c41-4afe-f5dd-e01b467394ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-347a07de33c9>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['freq'] = frequencies\n",
            "<ipython-input-17-347a07de33c9>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['freq'] = frequencies\n",
            "<ipython-input-17-347a07de33c9>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['freq'] = frequencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['src_port'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKbL63aMB6Q-",
        "outputId": "f8aa6786-af24-4021-d4d2-8525cc255deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7375"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = pd.DataFrame()"
      ],
      "metadata": {
        "id": "0nMFcEZVPEhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers['src_port'] = df.groupby(['src_ip'])[['src_port']].apply(is_outlier, 'src_port', src_outliers['src_port'])\n",
        "outliers['dest_port'] = df.groupby(['src_ip'])[['dest_port']].apply(is_outlier, 'dest_port', dest_outliers['dest_port'])\n",
        "outliers['sensor'] = df.groupby(['src_ip'])[['sensor']].apply(is_outlier, 'sensor', sensor_outliers['sensor'])"
      ],
      "metadata": {
        "id": "_lYaPZrKRECC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_col_names_o = ['num_outliers '+ i for i in outliers.columns]\n",
        "outliers.columns = new_col_names_o"
      ],
      "metadata": {
        "id": "0ht1vJt45qf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 3 most common values:\n",
        "\n",
        "# df[['src_port']].value_counts(dropna=True)[:3].index.tolist()\n",
        "src_common =  most_common(df, ['src_port'])\n",
        "dest_common =  most_common(df, ['dest_port'])\n",
        "sensor_common =  most_common(df, ['sensor'])"
      ],
      "metadata": {
        "id": "w9CZvA2254Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_common = pd.DataFrame()\n",
        "\n",
        "num_common['num_common src_port'] = df.groupby(['src_ip'])[['src_port']].apply(is_common, 'src_port', [40601,47001,54201])\n",
        "num_common['num_common dest_port'] = df.groupby(['src_ip'])[['dest_port']].apply(is_common, 'dest_port', [23, 445, 2222])\n",
        "num_common['num_common sensor'] = df.groupby(['src_ip'])[['sensor']].apply(is_common, 'sensor', ['db542e60-bcd5-422e-b627-c28f895858ea','40a6483a-70d1-42ab-88c7-c63eb7d4cea2','1d911ad4-0dac-4200-9abd-92ce03e7bf59'])"
      ],
      "metadata": {
        "id": "J0s3xJBq6N7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Entropy"
      ],
      "metadata": {
        "id": "EP0krSFScL0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get p distribution for calculating cross entropy\n",
        "psp = df.groupby('src_ip').apply(get_p, column = 'src_port')\n",
        "pdp = df.groupby('src_ip').apply(get_p, column = 'dest_port')\n",
        "pu = df.groupby('src_ip').apply(get_p, column = 'username')\n",
        "psshu = df.groupby('src_ip').apply(get_p, column = 'ssh_username')\n",
        "pc = df.groupby('src_ip').apply(get_p, column = 'command')"
      ],
      "metadata": {
        "id": "tjEm9cCtourw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparative_sp = df['src_port'].value_counts(dropna = True)\n",
        "\n",
        "df_qsp = df.groupby('src_ip').apply(get_qstep1, df2 = comparative_sp, column = 'src_port')\n",
        "df_qsp = df_qsp.replace(0,np.nan)"
      ],
      "metadata": {
        "id": "UWcH0ETtsctY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparative_dp = df['dest_port'].value_counts(dropna = True)\n",
        "\n",
        "df_qdp = df.groupby('src_ip').apply(get_qstep1, df2 = comparative_dp, column = 'dest_port')\n",
        "df_qdp = df_qdp.replace(0,np.nan)"
      ],
      "metadata": {
        "id": "Se4ZJA7FJUsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparative_u = df['username'].value_counts(dropna = True)\n",
        "\n",
        "df_qu = df.groupby('src_ip').apply(get_qstep1, df2 = comparative_u, column = 'username')\n",
        "df_qu = df_qu.replace(0,np.nan)"
      ],
      "metadata": {
        "id": "kVHyJK6UJay1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparative_sshu = df['ssh_username'].value_counts(dropna = True)\n",
        "\n",
        "df_qsshu = df.groupby('src_ip').apply(get_qstep1, df2 = comparative_sshu, column = 'ssh_username')\n",
        "df_qsshu = df_qsshu.replace(0,np.nan)"
      ],
      "metadata": {
        "id": "wwAjTR0fJbR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparative_c = df['command'].value_counts(dropna = True)\n",
        "\n",
        "df_qc = df.groupby('src_ip').apply(get_qstep1, df2 = comparative_c, column = 'command')\n",
        "df_qc = df_qc.replace(0,np.nan)"
      ],
      "metadata": {
        "id": "VsNJdntGJbjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ce_sp = ce_steps(psp, df_qsp, 'src_port')\n",
        "df_ce_dp = ce_steps(pdp, df_qdp, 'dest_port')\n",
        "df_ce_u = ce_steps(pu, df_qu, 'username')\n",
        "df_ce_sshu = ce_steps(psshu, df_qsshu, 'ssh_username')\n",
        "df_ce_c = ce_steps(pc, df_qc, 'command')"
      ],
      "metadata": {
        "id": "NAmtuZjDc1E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy_df = pd.concat([df_ce_sp, df_ce_dp, df_ce_u, df_ce_sshu, df_ce_c], axis=1)"
      ],
      "metadata": {
        "id": "jxKpgsNfeVNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy_df.columns = ['ce src_port', 'ce dest_port', 'ce username', 'ce ssh_username', 'ce command']"
      ],
      "metadata": {
        "id": "L5Hg9yUYxk5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_columns = ['src_port', 'dest_port', 'sensor', 'username', 'ssh_username', 'command']"
      ],
      "metadata": {
        "id": "hgKDFHZw61DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create final df"
      ],
      "metadata": {
        "id": "AUIJcopTSSwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformed features of the dataframe are used in unusual ips for processing and analysis.\n",
        "final_df_names = [attack_freq, entropy_dataset, number_valid_df, percent_valid_df, unique_valid_df, outliers, num_common, cross_entropy_df]"
      ],
      "metadata": {
        "id": "ZjvmbXvBR9oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.concat(final_df_names, axis=1)"
      ],
      "metadata": {
        "id": "6f9jvs8AiY0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}